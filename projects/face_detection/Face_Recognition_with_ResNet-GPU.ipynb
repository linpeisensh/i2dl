{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Face_Recognition_with_ResNet-Copy2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"5UwwC4pqnVVS","colab":{}},"source":["import copy\n","from os import listdir\n","from os.path import isfile, join\n","\n","import cv2\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import dlib\n","from imutils import face_utils\n","from random import shuffle\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_ZpkUDvanVVb","colab":{}},"source":["class Lambda(nn.Module):\n","    def __init__(self, func):\n","        super().__init__()\n","        self.func = func\n","\n","    def forward(self, x):\n","        return self.func(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"voDFWdWDnVVf","colab":{}},"source":["class ResidualBlock(nn.Module):\n","    \"\"\"\n","    The residual block used by ResNet.\n","    \n","    Args:\n","        in_channels: The number of channels (feature maps) of the incoming embedding\n","        out_channels: The number of channels after the first convolution\n","        stride: Stride size of the first convolution, used for downsampling\n","    \"\"\"\n","    \n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()        \n","        if stride > 1 or in_channels != out_channels:\n","            # Add strides in the skip connection and zeros for the new channels.\n","            self.skip = Lambda(lambda x: F.pad(x[:, :, ::stride, ::stride],\n","                                               (0, 0, 0, 0, 0, out_channels - in_channels),\n","                                               mode=\"constant\", value=0))\n","        else:\n","            self.skip = nn.Sequential()\n","            \n","        # TODO: Initialize the required layers\n","        self.layer = nn.Sequential(\n","                nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False),\n","                nn.BatchNorm2d(out_channels),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1,bias=False),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","    def forward(self, inputs):\n","        # TODO: Execute the required layers and functions\n","        out = self.layer(inputs)\n","        out += self.skip(inputs)\n","        out = F.relu(out)\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9OEZP4aynVVi","colab":{}},"source":["class ResidualStack(nn.Module):\n","    \"\"\"\n","    A stack of residual blocks.\n","    \n","    Args:\n","        in_channels: The number of channels (feature maps) of the incoming embedding\n","        out_channels: The number of channels after the first layer\n","        stride: Stride size of the first layer, used for downsampling\n","        num_blocks: Number of residual blocks\n","    \"\"\"\n","    \n","    def __init__(self, in_channels, out_channels, stride, num_blocks):\n","        super().__init__()\n","        \n","        # TODO: Initialize the required layers (blocks)\n","        layer = []\n","        layer.append(ResidualBlock(in_channels, out_channels, stride))\n","        for i in range(1,num_blocks):\n","            layer.append(ResidualBlock(out_channels, out_channels))\n","        self.layer = nn.Sequential(*layer)\n","\n","    def forward(self, out):\n","        # TODO: Execute the layers (blocks)\n","        out = self.layer(out)\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DEUQD9tMnVVm","colab":{}},"source":["class ResNet(nn.Module):\n","    def __init__(self, n = 5, num_classes = 2):\n","        super().__init__()\n","        self.n = 5\n","        self.num_classes = 2\n","\n","        # TODO: Implement ResNet via nn.Sequential\n","        self.layer = nn.Sequential(\n","        nn.Conv2d(3,64,3,1,1,bias=False),\n","        nn.BatchNorm2d(64),\n","        nn.ReLU(inplace=True),\n","        ResidualStack(64,64,1,n),\n","        ResidualStack(64,128,2,n),\n","        ResidualStack(128,256,2,n),\n","        ResidualStack(256,512,2,n),\n","        ResidualStack(512,1024,2,n),\n","        nn.AvgPool2d(8), \n","        Lambda(lambda x: torch.squeeze(x)),\n","        nn.Linear(1024,num_classes), \n","        )\n","    def forward(self, out):\n","        # TODO: Execute the layers (blocks)\n","        out = self.layer(out)\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"G2nasertpb6g","colab":{}},"source":["resnet = ResNet()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6O5RGi2bnVV3","colab":{}},"source":["normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","transform_train = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomCrop(128, 4),\n","    transforms.ToTensor(),\n","    normalize,\n","])\n","transform_eval = transforms.Compose([\n","    transforms.ToTensor(),\n","    normalize\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MloERP0PRTOL","colab":{}},"source":["path1 = '1/'\n","path0 = '0/'\n","files1 = [image for image in listdir(path1) if isfile(join(path1, image))]\n","files0 = [image for image in listdir(path0) if isfile(join(path0, image))]\n","shuffle(files1)\n","shuffle(files0)\n","\n","inputs = []\n","\n","for i, file in enumerate(files1):\n","    image_path = path1 + files1[i]\n","    img = Image.open(image_path)\n","    inputs.append(img)\n","\n","for i, file in enumerate(files0):\n","    image_path = path0 + files0[i]\n","    img = Image.open(image_path)\n","    inputs.append(img)\n","\n","targets1 = np.ones(len(files1), dtype=np.long)\n","targets0 = np.zeros(len(files0), dtype=np.long)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5hyvAQZJOcWs","colab_type":"code","colab":{}},"source":["# ntrain = 182\n","# nval = 26\n","ntrain = 360\n","nval = 72\n","ndata1 = len(files1)\n","train_set = []\n","val_set = []\n","test_set = []\n","for i in range(ntrain):\n","    train_set.append((transform_train(inputs[i]),targets1[i]))\n","    train_set.append((transform_train(inputs[i+ndata1]),targets0[i]))\n","for i in range(ntrain, ndata1):\n","    val_set.append((transform_eval(inputs[i]),targets1[i]))\n","    val_set.append((transform_eval(inputs[i+ndata1]),targets0[i]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_lJcQ_fLOXFo","colab":{}},"source":["dataloaders = {}\n","dataloaders['train'] = torch.utils.data.DataLoader(train_set, batch_size=8,\n","                                                   shuffle=True, num_workers=0,\n","                                                   pin_memory=True)\n","dataloaders['val'] = torch.utils.data.DataLoader(val_set, batch_size=8,\n","                                                 shuffle=False, num_workers=0,\n","                                                 pin_memory=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MgTa-0BmQ816","colab_type":"code","colab":{}},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","resnet.to(device);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"11bbU_XnnVWF","colab":{}},"source":["def run_epoch(model, optimizer, dataloader, train):\n","    \"\"\"\n","    Run one epoch of training or evaluation.\n","    \n","    Args:\n","        model: The model used for prediction\n","        optimizer: Optimization algorithm for the model\n","        dataloader: Dataloader providing the data to run our model on\n","        train: Whether this epoch is used for training or evaluation\n","        \n","    Returns:\n","        Loss and accuracy in this epoch.\n","    \"\"\"\n","    # TODO: Change the necessary parts to work correctly during evaluation (train=False)\n","    device = next(model.parameters()).device\n","\n","    epoch_loss = 0.0\n","    epoch_acc = 0.0\n","\n","    # Set model to training mode (for e.g. batch normalization, dropout)\n","    if train:\n","        model.train()\n","    else:\n","        model.eval()\n","\n","    for xb, yb in dataloader:\n","        xb, yb = xb.to(device), yb.to(device)\n","      # zero the parameter gradients\n","        if train:\n","            optimizer.zero_grad()\n","\n","      # forward\n","        with torch.set_grad_enabled(True):\n","            pred = model(xb.float())\n","            loss = F.cross_entropy(pred, yb.long())\n","            top1 = torch.argmax(pred, dim=1)\n","            ncorrect = torch.sum(top1 == yb)\n","\n","        if train:  \n","            loss.backward()\n","            optimizer.step()\n","\n","      # statistics\n","        epoch_loss += loss.item()\n","        epoch_acc += ncorrect.item()\n","\n","    epoch_loss /= len(dataloader.dataset)\n","    epoch_acc /= len(dataloader.dataset)\n","    return epoch_loss, epoch_acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B5la1f7-pQGn","colab":{}},"source":["def fit(model, optimizer, lr_scheduler, dataloaders, max_epochs, patience):\n","    \"\"\"\n","    Fit the given model on the dataset.\n","    \n","    Args:\n","        model: The model used for prediction\n","        optimizer: Optimization algorithm for the model\n","        lr_scheduler: Learning rate scheduler that improves training\n","                      in late epochs with learning rate decay\n","        dataloaders: Dataloaders for training and validation\n","        max_epochs: Maximum number of epochs for training\n","        patience: Number of epochs to wait with early stopping the\n","                  training if validation loss has decreased\n","                  \n","    Returns:\n","        Loss and accuracy in this epoch.\n","    \"\"\"\n","    \n","    best_acc = 0\n","    curr_patience = 0\n","    PATH = './data'\n","    for epoch in range(max_epochs):\n","        train_loss, train_acc = run_epoch(model, optimizer, dataloaders['train'], train=True)\n","        lr_scheduler.step()\n","        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, train loss: {train_loss:.2e}, accuracy: {train_acc * 100:.2f}%\")\n","\n","        val_loss, val_acc = run_epoch(model, None, dataloaders['val'], train=False)\n","        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, val loss: {val_loss:.2e}, accuracy: {val_acc * 100:.2f}%\")\n","\n","        # TODO: Add early stopping and save the best weights (in best_model_weights)\n","        if val_acc > best_acc:\n","            curr_patience = 0\n","            best_acc = val_acc\n","            best_model_weights = copy.deepcopy(model.state_dict())\n","        else:\n","            curr_patience += 1\n","        if curr_patience >= patience:\n","            break\n","    \n","    model.load_state_dict(best_model_weights)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6nG1qXbROj_L","outputId":"dc7cefe7-bbe0-4e43-c5c0-94cd7844a9ff","executionInfo":{"status":"ok","timestamp":1584526541825,"user_tz":-60,"elapsed":6228038,"user":{"displayName":"林培森","photoUrl":"","userId":"17885555047204399609"}},"colab":{"base_uri":"https://localhost:8080/","height":287}},"source":["optimizer = torch.optim.SGD(resnet.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n","lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n","\n","# Fit model\n","fit(resnet, optimizer, lr_scheduler, dataloaders, max_epochs=200, patience=15)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch   1/200, train loss: 2.60e+00, accuracy: 60.83%\n","Epoch   1/200, val loss: 6.74e-02, accuracy: 78.87%\n","Epoch   2/200, train loss: 6.04e-02, accuracy: 81.94%\n","Epoch   2/200, val loss: 2.34e-02, accuracy: 90.85%\n","Epoch   3/200, train loss: 2.68e-02, accuracy: 91.39%\n","Epoch   3/200, val loss: 1.45e-02, accuracy: 94.37%\n","Epoch   4/200, train loss: 3.22e-02, accuracy: 90.14%\n","Epoch   4/200, val loss: 2.20e-02, accuracy: 93.66%\n","Epoch   5/200, train loss: 1.96e-02, accuracy: 93.61%\n","Epoch   5/200, val loss: 1.31e-02, accuracy: 94.37%\n","Epoch   6/200, train loss: 2.32e-02, accuracy: 93.33%\n","Epoch   6/200, val loss: 1.36e-02, accuracy: 95.77%\n","Epoch   7/200, train loss: 1.51e-02, accuracy: 94.72%\n","Epoch   7/200, val loss: 1.34e-02, accuracy: 95.07%\n","Epoch   8/200, train loss: 8.86e-03, accuracy: 97.22%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nLZj80sc2a3L","colab":{}},"source":["torch.save(resnet.state_dict(), 'param_cpu.pkl')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5pgVhIZUOcXs","colab_type":"code","colab":{}},"source":["# resnet.load_state_dict(torch.load('param_640.pkl'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lVswATZjvy0x","colab":{}},"source":["def hisEqulColor(img):  \n","    ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)  \n","    channels = cv2.split(ycrcb)  \n","    cv2.equalizeHist(channels[0], channels[0]) #equalizeHist(in,out)  \n","    cv2.merge(channels, ycrcb)  \n","    img_eq=cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR)  \n","    return img_eq"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UCe8whfLV7q4","colab":{}},"source":["face_detect = dlib.get_frontal_face_detector()\n","path = 'test_image/'\n","files = [image for image in listdir(path) if isfile(join(path, image))]\n","device = next(resnet.parameters()).device\n","for i, file in enumerate(files):\n","    image_path = path + files[i]\n","    print(image_path)\n","    frame =  cv2.imread(image_path)\n","    img = hisEqulColor(frame)\n","    new_dimension = (128,128)\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    rects = face_detect(gray, 1)\n","    for (i, rect) in enumerate(rects):\n","        (x, y, w, h) = face_utils.rect_to_bb(rect)\n","        cropped_face =  frame[y:y+h, x:x+w]\n","        # resizing found face to new dimensions\n","        resized_face = cv2.resize(cropped_face, new_dimension)\n","        resized_face = cv2.cvtColor(resized_face,cv2.COLOR_BGR2RGB)\n","        resized_face = Image.fromarray(resized_face, 'RGB')\n","        inp = transform_eval(resized_face)\n","        plt.imshow(resized_face, cmap='gray')\n","        plt.axis('off')\n","        plt.show()\n","        x = torch.unsqueeze(inp,0)\n","        x = x.to(device)\n","        pred = resnet(x)\n","        top1 = torch.argmax(pred)\n","        print(pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"asANQPDYR0n8","colab":{}},"source":["def classifier(frame):\n","    face_detect = dlib.get_frontal_face_detector()\n","    img = hisEqulColor(frame)\n","    new_dimension = (128,128)\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #img\n","    rects = face_detect(gray, 1)\n","    cropped_face = None\n","    for (i, rect) in enumerate(rects):\n","        (x, y, w, h) = face_utils.rect_to_bb(rect)\n","        cropped_face = frame[y:y+h, x:x+w]\n","\n","    try:\n","        if cropped_face is not None:\n","        # resizing found face to new dimensions\n","            resized_face = cv2.resize(cropped_face, new_dimension)\n","            resized_face = cv2.cvtColor(resized_face,cv2.COLOR_BGR2RGB)\n","            resized_face = Image.fromarray(resized_face, 'RGB')\n","            inp = transform_eval(resized_face)\n","            x = torch.unsqueeze(inp,0)\n","            x = x.to(device)\n","            pred = resnet(x)\n","            top1 = torch.argmax(pred)\n","            return top1 \n","        else:\n","            return None\n","    except:\n","        return None"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uri8YQsZv77P","colab":{}},"source":["capture = cv2.VideoCapture('Blightt2.avi')\n","response, frame = capture.read()\n","fps = capture.get(cv2.CAP_PROP_FPS)\n","size = (int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)),\n","        int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n","writer = cv2.VideoWriter('Face_recognition.avi', cv2.VideoWriter_fourcc(*'XVID'), fps, size)\n","face_detect = dlib.get_frontal_face_detector()\n","res = []\n","while response:\n","    result = classifier(frame)\n","    if result == 1:\n","        res.append(1)        \n","        message = 'You you are finally here'\n","        cv2.putText(frame, message, (50,80), cv2.FONT_ITALIC, 1, (0,255,0), 2)\t\n","\n","    elif result == 0:\n","        res.append(0)\n","        message = 'Sorry! You are not who you say'\n","        cv2.putText(frame, message, (50,80), cv2.FONT_ITALIC, 1, (0,0,255), 2)\n","        \n","    else:\n","        message = 'No face found!'\n","        cv2.putText(frame, message, (50,80), cv2.FONT_ITALIC, 1, (0,200,200), 2)\n","\n","\n","    writer.write(frame)\n","    response, frame = capture.read()\n","print(np.mean(res))\n","capture.release()\n","writer.release()\n","cv2.destroyAllWindows()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hTwhqOy26Um","colab_type":"code","colab":{}},"source":["capture = cv2.VideoCapture('Blightt1.avi')\n","response, frame = capture.read()\n","fps = capture.get(cv2.CAP_PROP_FPS)\n","size = (int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)),\n","        int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n","face_detect = dlib.get_frontal_face_detector()\n","res = []\n","count = 0\n","count_n = 0\n","while response:\n","    count += 1\n","    result = classifier(frame)\n","    if result == 1:\n","        res.append(1)        \n","\n","    elif result == 0:\n","        res.append(0)\n","        \n","    else:\n","        count_n += 1\n","\n","\n","    response, frame = capture.read()\n","print(np.mean(res))\n","print(1-count_n/count)\n","capture.release()\n","cv2.destroyAllWindows()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KyMS884VOcYy","colab_type":"code","colab":{}},"source":["capture = cv2.VideoCapture('original3.mp4')\n","response, frame = capture.read()\n","fps = capture.get(cv2.CAP_PROP_FPS)\n","size = (int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)),\n","        int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n","face_detect = dlib.get_frontal_face_detector()\n","res = []\n","count = 0\n","count_n = 0\n","while response:\n","    count += 1\n","    result = classifier(frame)\n","    if result == 1:\n","        res.append(1)        \n","\n","    elif result == 0:\n","        res.append(0)\n","        \n","    else:\n","        count_n += 1\n","\n","\n","    response, frame = capture.read()\n","print(np.mean(res))\n","print(1-count_n/count)\n","capture.release()\n","cv2.destroyAllWindows()"],"execution_count":0,"outputs":[]}]}