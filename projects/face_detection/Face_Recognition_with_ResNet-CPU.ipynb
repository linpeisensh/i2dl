{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5UwwC4pqnVVS"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZpkUDvanVVb"
   },
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "voDFWdWDnVVf"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The residual block used by ResNet.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: The number of channels (feature maps) of the incoming embedding\n",
    "        out_channels: The number of channels after the first convolution\n",
    "        stride: Stride size of the first convolution, used for downsampling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()        \n",
    "        if stride > 1 or in_channels != out_channels:\n",
    "            # Add strides in the skip connection and zeros for the new channels.\n",
    "            self.skip = Lambda(lambda x: F.pad(x[:, :, ::stride, ::stride],\n",
    "                                               (0, 0, 0, 0, 0, out_channels - in_channels),\n",
    "                                               mode=\"constant\", value=0))\n",
    "        else:\n",
    "            self.skip = nn.Sequential()\n",
    "            \n",
    "        # TODO: Initialize the required layers\n",
    "        self.layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    def forward(self, inputs):\n",
    "        # TODO: Execute the required layers and functions\n",
    "        out = self.layer(inputs)\n",
    "        out += self.skip(inputs)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OEZP4aynVVi"
   },
   "outputs": [],
   "source": [
    "class ResidualStack(nn.Module):\n",
    "    \"\"\"\n",
    "    A stack of residual blocks.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: The number of channels (feature maps) of the incoming embedding\n",
    "        out_channels: The number of channels after the first layer\n",
    "        stride: Stride size of the first layer, used for downsampling\n",
    "        num_blocks: Number of residual blocks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride, num_blocks):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Initialize the required layers (blocks)\n",
    "        layer = []\n",
    "        layer.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        for i in range(1,num_blocks):\n",
    "            layer.append(ResidualBlock(out_channels, out_channels))\n",
    "        self.layer = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, out):\n",
    "        # TODO: Execute the layers (blocks)\n",
    "        out = self.layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEUQD9tMnVVm"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, n = 5, num_classes = 2):\n",
    "        super().__init__()\n",
    "        self.n = 5\n",
    "        self.num_classes = 2\n",
    "\n",
    "        # TODO: Implement ResNet via nn.Sequential\n",
    "        self.layer = nn.Sequential(\n",
    "        nn.Conv2d(3,16,3,1,1,bias=False),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ReLU(inplace=True),\n",
    "        ResidualStack(16,16,1,n),\n",
    "        ResidualStack(16,32,2,n),\n",
    "        ResidualStack(32,64,2,n),\n",
    "        ResidualStack(64,64,2,n),\n",
    "        ResidualStack(64,64,2,n),\n",
    "        nn.AvgPool2d(8), \n",
    "        Lambda(lambda x: torch.squeeze(x)),\n",
    "        nn.Linear(64,num_classes), \n",
    "        )\n",
    "    def forward(self, out):\n",
    "        # TODO: Execute the layers (blocks)\n",
    "        out = self.layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2nasertpb6g"
   },
   "outputs": [],
   "source": [
    "resnet = ResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6O5RGi2bnVV3"
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(128, 4),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MloERP0PRTOL"
   },
   "outputs": [],
   "source": [
    "path1 = 'trainingData1/'\n",
    "path0 = 'trainingData0/'\n",
    "files1 = [image for image in listdir(path1) if isfile(join(path1, image))]\n",
    "files0 = [image for image in listdir(path0) if isfile(join(path0, image))]\n",
    "shuffle(files1)\n",
    "shuffle(files0)\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for i, file in enumerate(files1):\n",
    "    image_path = path1 + files1[i]\n",
    "    img = Image.open(image_path)\n",
    "    inputs.append(img)\n",
    "\n",
    "for i, file in enumerate(files0):\n",
    "    image_path = path0 + files0[i]\n",
    "    img = Image.open(image_path)\n",
    "    inputs.append(img)\n",
    "\n",
    "targets1 = np.ones(len(files1), dtype=np.long)\n",
    "targets0 = np.zeros(len(files0), dtype=np.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 182\n",
    "nval = 26\n",
    "ndata1 = len(files1)\n",
    "train_set = []\n",
    "val_set = []\n",
    "test_set = []\n",
    "for i in range(ntrain):\n",
    "    train_set.append((transform_train(inputs[i]),targets1[i]))\n",
    "    train_set.append((transform_train(inputs[i+ndata1]),targets0[i]))\n",
    "for i in range(ntrain, ndata1):\n",
    "    val_set.append((transform_eval(inputs[i]),targets1[i]))\n",
    "    val_set.append((transform_eval(inputs[i+ndata1]),targets0[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lJcQ_fLOXFo"
   },
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(train_set, batch_size=8,\n",
    "                                                   shuffle=True, num_workers=0,\n",
    "                                                   pin_memory=True)\n",
    "dataloaders['val'] = torch.utils.data.DataLoader(val_set, batch_size=8,\n",
    "                                                 shuffle=False, num_workers=0,\n",
    "                                                 pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "11bbU_XnnVWF"
   },
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, dataloader, train):\n",
    "    \"\"\"\n",
    "    Run one epoch of training or evaluation.\n",
    "    \n",
    "    Args:\n",
    "        model: The model used for prediction\n",
    "        optimizer: Optimization algorithm for the model\n",
    "        dataloader: Dataloader providing the data to run our model on\n",
    "        train: Whether this epoch is used for training or evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Loss and accuracy in this epoch.\n",
    "    \"\"\"\n",
    "    # TODO: Change the necessary parts to work correctly during evaluation (train=False)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "    # Set model to training mode (for e.g. batch normalization, dropout)\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for xb, yb in dataloader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "      # zero the parameter gradients\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "      # forward\n",
    "        with torch.set_grad_enabled(True):\n",
    "            pred = model(xb.float())\n",
    "            loss = F.cross_entropy(pred, yb.long())\n",
    "            top1 = torch.argmax(pred, dim=1)\n",
    "            ncorrect = torch.sum(top1 == yb)\n",
    "\n",
    "        if train:  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "      # statistics\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += ncorrect.item()\n",
    "\n",
    "    epoch_loss /= len(dataloader.dataset)\n",
    "    epoch_acc /= len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5la1f7-pQGn"
   },
   "outputs": [],
   "source": [
    "def fit(model, optimizer, lr_scheduler, dataloaders, max_epochs, patience):\n",
    "    \"\"\"\n",
    "    Fit the given model on the dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The model used for prediction\n",
    "        optimizer: Optimization algorithm for the model\n",
    "        lr_scheduler: Learning rate scheduler that improves training\n",
    "                      in late epochs with learning rate decay\n",
    "        dataloaders: Dataloaders for training and validation\n",
    "        max_epochs: Maximum number of epochs for training\n",
    "        patience: Number of epochs to wait with early stopping the\n",
    "                  training if validation loss has decreased\n",
    "                  \n",
    "    Returns:\n",
    "        Loss and accuracy in this epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_acc = 0\n",
    "    curr_patience = 0\n",
    "    PATH = './data'\n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss, train_acc = run_epoch(model, optimizer, dataloaders['train'], train=True)\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, train loss: {train_loss:.2e}, accuracy: {train_acc * 100:.2f}%\")\n",
    "\n",
    "        val_loss, val_acc = run_epoch(model, None, dataloaders['val'], train=False)\n",
    "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, val loss: {val_loss:.2e}, accuracy: {val_acc * 100:.2f}%\")\n",
    "\n",
    "        # TODO: Add early stopping and save the best weights (in best_model_weights)\n",
    "        if val_acc > best_acc:\n",
    "            curr_patience = 0\n",
    "            best_acc = val_acc\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            curr_patience += 1\n",
    "        if curr_patience >= patience:\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nG1qXbROj_L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/200, train loss: 4.92e-01, accuracy: 61.81%\n",
      "Epoch   1/200, val loss: 8.37e-01, accuracy: 71.25%\n",
      "Epoch   2/200, train loss: 7.43e-02, accuracy: 75.00%\n",
      "Epoch   2/200, val loss: 1.44e-01, accuracy: 50.00%\n",
      "Epoch   3/200, train loss: 5.47e-02, accuracy: 85.44%\n",
      "Epoch   3/200, val loss: 3.71e-02, accuracy: 83.75%\n",
      "Epoch   4/200, train loss: 2.32e-02, accuracy: 93.68%\n",
      "Epoch   4/200, val loss: 8.27e-03, accuracy: 97.50%\n",
      "Epoch   5/200, train loss: 3.16e-02, accuracy: 93.13%\n",
      "Epoch   5/200, val loss: 1.90e-01, accuracy: 50.00%\n",
      "Epoch   6/200, train loss: 2.46e-02, accuracy: 93.41%\n",
      "Epoch   6/200, val loss: 1.00e-02, accuracy: 96.25%\n",
      "Epoch   7/200, train loss: 2.16e-02, accuracy: 94.78%\n",
      "Epoch   7/200, val loss: 7.00e-03, accuracy: 98.75%\n",
      "Epoch   8/200, train loss: 1.39e-02, accuracy: 96.43%\n",
      "Epoch   8/200, val loss: 8.60e-03, accuracy: 98.75%\n",
      "Epoch   9/200, train loss: 1.91e-02, accuracy: 95.33%\n",
      "Epoch   9/200, val loss: 1.56e-02, accuracy: 97.50%\n",
      "Epoch  10/200, train loss: 1.92e-02, accuracy: 95.88%\n",
      "Epoch  10/200, val loss: 1.12e-02, accuracy: 97.50%\n",
      "Epoch  11/200, train loss: 1.28e-02, accuracy: 96.70%\n",
      "Epoch  11/200, val loss: 1.42e-02, accuracy: 98.75%\n",
      "Epoch  12/200, train loss: 1.11e-02, accuracy: 97.25%\n",
      "Epoch  12/200, val loss: 2.12e-01, accuracy: 50.00%\n",
      "Epoch  13/200, train loss: 2.16e-02, accuracy: 95.05%\n",
      "Epoch  13/200, val loss: 6.84e-03, accuracy: 98.75%\n",
      "Epoch  14/200, train loss: 9.52e-03, accuracy: 97.53%\n",
      "Epoch  14/200, val loss: 2.42e-03, accuracy: 100.00%\n",
      "Epoch  15/200, train loss: 1.04e-02, accuracy: 97.53%\n",
      "Epoch  15/200, val loss: 7.45e-03, accuracy: 97.50%\n",
      "Epoch  16/200, train loss: 6.42e-03, accuracy: 98.63%\n",
      "Epoch  16/200, val loss: 2.75e-03, accuracy: 98.75%\n",
      "Epoch  17/200, train loss: 5.61e-03, accuracy: 98.90%\n",
      "Epoch  17/200, val loss: 1.19e-03, accuracy: 100.00%\n",
      "Epoch  18/200, train loss: 5.79e-03, accuracy: 98.35%\n",
      "Epoch  18/200, val loss: 1.19e-03, accuracy: 100.00%\n",
      "Epoch  19/200, train loss: 5.46e-03, accuracy: 99.18%\n",
      "Epoch  19/200, val loss: 8.51e-04, accuracy: 100.00%\n",
      "Epoch  20/200, train loss: 8.91e-03, accuracy: 97.53%\n",
      "Epoch  20/200, val loss: 2.38e-01, accuracy: 50.00%\n",
      "Epoch  21/200, train loss: 9.38e-03, accuracy: 97.80%\n",
      "Epoch  21/200, val loss: 1.26e-02, accuracy: 97.50%\n",
      "Epoch  22/200, train loss: 2.06e-02, accuracy: 96.43%\n",
      "Epoch  22/200, val loss: 3.60e-03, accuracy: 100.00%\n",
      "Epoch  23/200, train loss: 8.66e-03, accuracy: 98.35%\n",
      "Epoch  23/200, val loss: 1.36e-03, accuracy: 100.00%\n",
      "Epoch  24/200, train loss: 6.17e-03, accuracy: 98.90%\n",
      "Epoch  24/200, val loss: 1.73e-03, accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
    "\n",
    "# Fit model\n",
    "fit(resnet, optimizer, lr_scheduler, dataloaders, max_epochs=200, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLZj80sc2a3L"
   },
   "outputs": [],
   "source": [
    "torch.save(resnet.state_dict(), 'param_cpu.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet.load_state_dict(torch.load('param_640.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lVswATZjvy0x"
   },
   "outputs": [],
   "source": [
    "def hisEqulColor(img):  \n",
    "    ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)  \n",
    "    channels = cv2.split(ycrcb)  \n",
    "    cv2.equalizeHist(channels[0], channels[0]) #equalizeHist(in,out)  \n",
    "    cv2.merge(channels, ycrcb)  \n",
    "    img_eq=cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR)  \n",
    "    return img_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UCe8whfLV7q4"
   },
   "outputs": [],
   "source": [
    "face_detect = dlib.get_frontal_face_detector()\n",
    "path = 'test_image/'\n",
    "files = [image for image in listdir(path) if isfile(join(path, image))]\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    image_path = path + files[i]\n",
    "    print(image_path)\n",
    "    frame =  cv2.imread(image_path)\n",
    "    img = hisEqulColor(frame)\n",
    "    new_dimension = (128,128)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    rects = face_detect(gray, 1)\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        cropped_face =  frame[y:y+h, x:x+w]\n",
    "        # resizing found face to new dimensions\n",
    "        resized_face = cv2.resize(cropped_face, new_dimension)\n",
    "        resized_face = cv2.cvtColor(resized_face,cv2.COLOR_BGR2RGB)\n",
    "        resized_face = Image.fromarray(resized_face, 'RGB')\n",
    "        inp = transform_eval(resized_face)\n",
    "        plt.imshow(resized_face, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        pred = resnet(torch.unsqueeze(inp,0))\n",
    "        top1 = torch.argmax(pred)\n",
    "        print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "asANQPDYR0n8"
   },
   "outputs": [],
   "source": [
    "def classifier(frame):\n",
    "    face_detect = dlib.get_frontal_face_detector()\n",
    "    img = hisEqulColor(frame)\n",
    "    new_dimension = (128,128)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #img\n",
    "    rects = face_detect(gray, 1)\n",
    "    cropped_face = None\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        cropped_face = frame[y:y+h, x:x+w]\n",
    "\n",
    "    try:\n",
    "        if cropped_face is not None:\n",
    "        # resizing found face to new dimensions\n",
    "            resized_face = cv2.resize(cropped_face, new_dimension)\n",
    "            resized_face = cv2.cvtColor(resized_face,cv2.COLOR_BGR2RGB)\n",
    "            resized_face = Image.fromarray(resized_face, 'RGB')\n",
    "            inp = transform_eval(resized_face)\n",
    "            pred = resnet(torch.unsqueeze(inp,0))\n",
    "            top1 = torch.argmax(pred)\n",
    "            return top1 \n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uri8YQsZv77P"
   },
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('test_video/Blightt2.avi')\n",
    "response, frame = capture.read()\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "size = (int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "writer = cv2.VideoWriter('Face_recognition.avi', cv2.VideoWriter_fourcc(*'XVID'), fps, size)\n",
    "face_detect = dlib.get_frontal_face_detector()\n",
    "res = []\n",
    "while response:\n",
    "    result = classifier(frame)\n",
    "    if result == 1:\n",
    "        res.append(1)        \n",
    "        message = 'You you are finally here'\n",
    "        cv2.putText(frame, message, (50,80), cv2.FONT_ITALIC, 1, (0,255,0), 2)\t\n",
    "\n",
    "    elif result == 0:\n",
    "        res.append(0)\n",
    "        message = 'Sorry! You are not who you say'\n",
    "        cv2.putText(frame, message, (50,80), cv2.FONT_ITALIC, 1, (0,0,255), 2)\n",
    "        \n",
    "    else:\n",
    "        message = 'No face found!'\n",
    "        cv2.putText(frame, message, (50,80), cv2.FONT_ITALIC, 1, (0,200,200), 2)\n",
    "\n",
    "\n",
    "    writer.write(frame)\n",
    "    response, frame = capture.read()\n",
    "\n",
    "capture.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('test_video/original3.mp4')\n",
    "response, frame = capture.read()\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "size = (int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "face_detect = dlib.get_frontal_face_detector()\n",
    "res = []\n",
    "count = 0\n",
    "count_n = 0\n",
    "while response:\n",
    "    count += 1\n",
    "    result = classifier(frame)\n",
    "    if result == 1:\n",
    "        res.append(1)        \n",
    "\n",
    "    elif result == 0:\n",
    "        res.append(0)\n",
    "        \n",
    "    else:\n",
    "        count_n += 1\n",
    "\n",
    "\n",
    "    response, frame = capture.read()\n",
    "print(np.mean(res))\n",
    "print(1-count_n/count)\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of ResNet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
