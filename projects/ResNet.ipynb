{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ResNet.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d144a761d67942b58fbd04df17462c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_29cbbe4c2a7e4a0c8ad2b35b1f90e647",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6fdc2f478c274f4aaef222f56e1ef879",
              "IPY_MODEL_00c908cd77e34232bb4771f205264564"
            ]
          }
        },
        "29cbbe4c2a7e4a0c8ad2b35b1f90e647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6fdc2f478c274f4aaef222f56e1ef879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_59f7881cecc24014b1ab7d4d258d665e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3af58fc7eff5430abe8743c2ed8c2664"
          }
        },
        "00c908cd77e34232bb4771f205264564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_efd8ebd19393466dab4f49cacae0c7fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "170500096it [00:30, 14123679.71it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_356c2e7f7fbb4c7aaf7abef07f167e66"
          }
        },
        "59f7881cecc24014b1ab7d4d258d665e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3af58fc7eff5430abe8743c2ed8c2664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "efd8ebd19393466dab4f49cacae0c7fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "356c2e7f7fbb4c7aaf7abef07f167e66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UwwC4pqnVVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb6FqXqinzqf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "1745a605-ff2a-4ad9-a85e-f7893426e2f2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZpkUDvanVVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.func(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voDFWdWDnVVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The residual block used by ResNet.\n",
        "    \n",
        "    Args:\n",
        "        in_channels: The number of channels (feature maps) of the incoming embedding\n",
        "        out_channels: The number of channels after the first convolution\n",
        "        stride: Stride size of the first convolution, used for downsampling\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()        \n",
        "        if stride > 1 or in_channels != out_channels:\n",
        "            # Add strides in the skip connection and zeros for the new channels.\n",
        "            self.skip = Lambda(lambda x: F.pad(x[:, :, ::stride, ::stride],\n",
        "                                               (0, 0, 0, 0, 0, out_channels - in_channels),\n",
        "                                               mode=\"constant\", value=0))\n",
        "        else:\n",
        "            self.skip = nn.Sequential()\n",
        "            \n",
        "        # TODO: Initialize the required layers\n",
        "        self.layer = nn.Sequential(\n",
        "                nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1,bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "    def forward(self, inputs):\n",
        "        # TODO: Execute the required layers and functions\n",
        "        out = self.layer(inputs)\n",
        "        out += self.skip(inputs)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OEZP4aynVVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualStack(nn.Module):\n",
        "    \"\"\"\n",
        "    A stack of residual blocks.\n",
        "    \n",
        "    Args:\n",
        "        in_channels: The number of channels (feature maps) of the incoming embedding\n",
        "        out_channels: The number of channels after the first layer\n",
        "        stride: Stride size of the first layer, used for downsampling\n",
        "        num_blocks: Number of residual blocks\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride, num_blocks):\n",
        "        super().__init__()\n",
        "        \n",
        "        # TODO: Initialize the required layers (blocks)\n",
        "        layer = []\n",
        "        layer.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        for i in range(1,num_blocks):\n",
        "            layer.append(ResidualBlock(out_channels, out_channels))\n",
        "        self.layer = nn.Sequential(*layer)\n",
        "\n",
        "    def forward(self, out):\n",
        "        # TODO: Execute the layers (blocks)\n",
        "        out = self.layer(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEUQD9tMnVVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 5\n",
        "num_classes = 10\n",
        "\n",
        "# TODO: Implement ResNet via nn.Sequential\n",
        "resnet = nn.Sequential(\n",
        "    nn.Conv2d(3,16,3,1,1,bias=False),\n",
        "    nn.BatchNorm2d(16),\n",
        "    nn.ReLU(inplace=True),\n",
        "    ResidualStack(16,16,1,n),\n",
        "    ResidualStack(16,32,2,n),\n",
        "    ResidualStack(32,64,2,n),\n",
        "    nn.AvgPool2d(8),\n",
        "    Lambda(lambda x: torch.squeeze(x)),\n",
        "    nn.Linear(64,num_classes),\n",
        "    # nn.Softmax(dim=1) \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJYNOknUnVVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_weight(module):\n",
        "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "    elif isinstance(module, nn.BatchNorm2d):\n",
        "        nn.init.constant_(module.weight, 1)\n",
        "        nn.init.constant_(module.bias, 0)\n",
        "        \n",
        "resnet.apply(initialize_weight);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1sZSy77nVVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CIFAR10Subset(torchvision.datasets.CIFAR10):\n",
        "    \"\"\"\n",
        "    Get a subset of the CIFAR10 dataset, according to the passed indices.\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, idx=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        \n",
        "        if idx is None:\n",
        "            return\n",
        "        \n",
        "        self.data = self.data[idx]\n",
        "        targets_np = np.array(self.targets)\n",
        "        self.targets = targets_np[idx].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O5RGi2bnVV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, 4),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuYh8_shnVV7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "d144a761d67942b58fbd04df17462c88",
            "29cbbe4c2a7e4a0c8ad2b35b1f90e647",
            "6fdc2f478c274f4aaef222f56e1ef879",
            "00c908cd77e34232bb4771f205264564",
            "59f7881cecc24014b1ab7d4d258d665e",
            "3af58fc7eff5430abe8743c2ed8c2664",
            "efd8ebd19393466dab4f49cacae0c7fb",
            "356c2e7f7fbb4c7aaf7abef07f167e66"
          ]
        },
        "outputId": "6c6e5fd8-1d8d-4278-bc08-c54e04611304"
      },
      "source": [
        "ntrain = 45_000\n",
        "train_set = CIFAR10Subset(root='./data', train=True, idx=range(ntrain),\n",
        "                          download=True, transform=transform_train)\n",
        "val_set = CIFAR10Subset(root='./data', train=True, idx=range(ntrain, 50_000),\n",
        "                        download=True, transform=transform_eval)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform_eval)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d144a761d67942b58fbd04df17462c88",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ7RBdATn8Fx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloaders = {}\n",
        "dataloaders['train'] = torch.utils.data.DataLoader(train_set, batch_size=128,\n",
        "                                                   shuffle=True, num_workers=2,\n",
        "                                                   pin_memory=True)\n",
        "dataloaders['val'] = torch.utils.data.DataLoader(val_set, batch_size=128,\n",
        "                                                 shuffle=False, num_workers=2,\n",
        "                                                 pin_memory=True)\n",
        "dataloaders['test'] = torch.utils.data.DataLoader(test_set, batch_size=128,\n",
        "                                                  shuffle=False, num_workers=2,\n",
        "                                                  pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwuJ1CYWnVV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "resnet.to(device);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11bbU_XnnVWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_epoch(model, optimizer, dataloader, train):\n",
        "    \"\"\"\n",
        "    Run one epoch of training or evaluation.\n",
        "    \n",
        "    Args:\n",
        "        model: The model used for prediction\n",
        "        optimizer: Optimization algorithm for the model\n",
        "        dataloader: Dataloader providing the data to run our model on\n",
        "        train: Whether this epoch is used for training or evaluation\n",
        "        \n",
        "    Returns:\n",
        "        Loss and accuracy in this epoch.\n",
        "    \"\"\"\n",
        "    # TODO: Change the necessary parts to work correctly during evaluation (train=False)\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "\n",
        "    # Set model to training mode (for e.g. batch normalization, dropout)\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    for xb, yb in dataloader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "      # zero the parameter gradients\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "      # forward\n",
        "        with torch.set_grad_enabled(True):\n",
        "            pred = model(xb)\n",
        "            loss = F.cross_entropy(pred, yb)\n",
        "            top1 = torch.argmax(pred, dim=1)\n",
        "            ncorrect = torch.sum(top1 == yb)\n",
        "\n",
        "        if train:  \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "      # statistics\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += ncorrect.item()\n",
        "\n",
        "    epoch_loss /= len(dataloader.dataset)\n",
        "    epoch_acc /= len(dataloader.dataset)\n",
        "    return epoch_loss, epoch_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr1fe5S0nVWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(model, optimizer, lr_scheduler, dataloaders, max_epochs, patience):\n",
        "    \"\"\"\n",
        "    Fit the given model on the dataset.\n",
        "    \n",
        "    Args:\n",
        "        model: The model used for prediction\n",
        "        optimizer: Optimization algorithm for the model\n",
        "        lr_scheduler: Learning rate scheduler that improves training\n",
        "                      in late epochs with learning rate decay\n",
        "        dataloaders: Dataloaders for training and validation\n",
        "        max_epochs: Maximum number of epochs for training\n",
        "        patience: Number of epochs to wait with early stopping the\n",
        "                  training if validation loss has decreased\n",
        "                  \n",
        "    Returns:\n",
        "        Loss and accuracy in this epoch.\n",
        "    \"\"\"\n",
        "    \n",
        "    best_acc = 0\n",
        "    curr_patience = 0\n",
        "    PATH = './data'\n",
        "    for epoch in range(max_epochs):\n",
        "        train_loss, train_acc = run_epoch(model, optimizer, dataloaders['train'], train=True)\n",
        "        lr_scheduler.step()\n",
        "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, train loss: {train_loss:.2e}, accuracy: {train_acc * 100:.2f}%\")\n",
        "\n",
        "        val_loss, val_acc = run_epoch(model, None, dataloaders['val'], train=False)\n",
        "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, val loss: {val_loss:.2e}, accuracy: {val_acc * 100:.2f}%\")\n",
        "\n",
        "        # TODO: Add early stopping and save the best weights (in best_model_weights)\n",
        "        if val_acc > best_acc:\n",
        "            curr_patience = 0\n",
        "            best_acc = val_acc\n",
        "            best_model_weights = copy.deepcopy(model.state_dict())\n",
        "        else:\n",
        "            curr_patience += 1\n",
        "        if curr_patience >= patience:\n",
        "            break\n",
        "    \n",
        "    model.load_state_dict(best_model_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI8GI4tCnVWM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72f5dd39-da14-49f3-ee93-a126c022037c"
      },
      "source": [
        "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
        "\n",
        "# Fit model\n",
        "fit(resnet, optimizer, lr_scheduler, dataloaders, max_epochs=200, patience=50)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   1/200, train loss: 1.41e-02, accuracy: 34.59%\n",
            "Epoch   1/200, val loss: 1.31e-02, accuracy: 39.54%\n",
            "Epoch   2/200, train loss: 9.69e-03, accuracy: 54.78%\n",
            "Epoch   2/200, val loss: 9.48e-03, accuracy: 58.80%\n",
            "Epoch   3/200, train loss: 7.65e-03, accuracy: 65.06%\n",
            "Epoch   3/200, val loss: 6.87e-03, accuracy: 70.58%\n",
            "Epoch   4/200, train loss: 6.33e-03, accuracy: 71.52%\n",
            "Epoch   4/200, val loss: 6.77e-03, accuracy: 71.40%\n",
            "Epoch   5/200, train loss: 5.53e-03, accuracy: 75.38%\n",
            "Epoch   5/200, val loss: 5.65e-03, accuracy: 76.20%\n",
            "Epoch   6/200, train loss: 5.00e-03, accuracy: 77.77%\n",
            "Epoch   6/200, val loss: 6.95e-03, accuracy: 72.02%\n",
            "Epoch   7/200, train loss: 4.62e-03, accuracy: 79.42%\n",
            "Epoch   7/200, val loss: 5.40e-03, accuracy: 77.36%\n",
            "Epoch   8/200, train loss: 4.27e-03, accuracy: 81.02%\n",
            "Epoch   8/200, val loss: 5.27e-03, accuracy: 79.20%\n",
            "Epoch   9/200, train loss: 4.00e-03, accuracy: 82.33%\n",
            "Epoch   9/200, val loss: 4.46e-03, accuracy: 82.18%\n",
            "Epoch  10/200, train loss: 3.82e-03, accuracy: 83.12%\n",
            "Epoch  10/200, val loss: 5.87e-03, accuracy: 75.80%\n",
            "Epoch  11/200, train loss: 3.66e-03, accuracy: 83.81%\n",
            "Epoch  11/200, val loss: 3.86e-03, accuracy: 84.42%\n",
            "Epoch  12/200, train loss: 3.52e-03, accuracy: 84.50%\n",
            "Epoch  12/200, val loss: 4.34e-03, accuracy: 82.38%\n",
            "Epoch  13/200, train loss: 3.39e-03, accuracy: 85.11%\n",
            "Epoch  13/200, val loss: 4.18e-03, accuracy: 83.38%\n",
            "Epoch  14/200, train loss: 3.27e-03, accuracy: 85.37%\n",
            "Epoch  14/200, val loss: 4.26e-03, accuracy: 83.06%\n",
            "Epoch  15/200, train loss: 3.14e-03, accuracy: 86.10%\n",
            "Epoch  15/200, val loss: 5.13e-03, accuracy: 80.08%\n",
            "Epoch  16/200, train loss: 2.99e-03, accuracy: 86.89%\n",
            "Epoch  16/200, val loss: 4.90e-03, accuracy: 81.02%\n",
            "Epoch  17/200, train loss: 2.93e-03, accuracy: 87.02%\n",
            "Epoch  17/200, val loss: 4.93e-03, accuracy: 80.64%\n",
            "Epoch  18/200, train loss: 2.87e-03, accuracy: 87.24%\n",
            "Epoch  18/200, val loss: 4.14e-03, accuracy: 84.20%\n",
            "Epoch  19/200, train loss: 2.77e-03, accuracy: 87.63%\n",
            "Epoch  19/200, val loss: 4.06e-03, accuracy: 84.16%\n",
            "Epoch  20/200, train loss: 2.73e-03, accuracy: 87.84%\n",
            "Epoch  20/200, val loss: 4.56e-03, accuracy: 81.84%\n",
            "Epoch  21/200, train loss: 2.63e-03, accuracy: 88.32%\n",
            "Epoch  21/200, val loss: 4.65e-03, accuracy: 81.78%\n",
            "Epoch  22/200, train loss: 2.59e-03, accuracy: 88.44%\n",
            "Epoch  22/200, val loss: 3.80e-03, accuracy: 84.82%\n",
            "Epoch  23/200, train loss: 2.51e-03, accuracy: 88.72%\n",
            "Epoch  23/200, val loss: 3.54e-03, accuracy: 85.86%\n",
            "Epoch  24/200, train loss: 2.48e-03, accuracy: 89.00%\n",
            "Epoch  24/200, val loss: 3.20e-03, accuracy: 87.08%\n",
            "Epoch  25/200, train loss: 2.43e-03, accuracy: 89.07%\n",
            "Epoch  25/200, val loss: 4.15e-03, accuracy: 84.04%\n",
            "Epoch  26/200, train loss: 2.32e-03, accuracy: 89.62%\n",
            "Epoch  26/200, val loss: 3.43e-03, accuracy: 86.58%\n",
            "Epoch  27/200, train loss: 2.32e-03, accuracy: 89.64%\n",
            "Epoch  27/200, val loss: 4.48e-03, accuracy: 82.44%\n",
            "Epoch  28/200, train loss: 2.30e-03, accuracy: 89.70%\n",
            "Epoch  28/200, val loss: 3.27e-03, accuracy: 86.66%\n",
            "Epoch  29/200, train loss: 2.24e-03, accuracy: 90.10%\n",
            "Epoch  29/200, val loss: 3.94e-03, accuracy: 84.96%\n",
            "Epoch  30/200, train loss: 2.21e-03, accuracy: 90.15%\n",
            "Epoch  30/200, val loss: 3.36e-03, accuracy: 85.86%\n",
            "Epoch  31/200, train loss: 2.14e-03, accuracy: 90.49%\n",
            "Epoch  31/200, val loss: 5.37e-03, accuracy: 81.00%\n",
            "Epoch  32/200, train loss: 2.13e-03, accuracy: 90.51%\n",
            "Epoch  32/200, val loss: 3.72e-03, accuracy: 85.76%\n",
            "Epoch  33/200, train loss: 2.04e-03, accuracy: 90.84%\n",
            "Epoch  33/200, val loss: 4.08e-03, accuracy: 83.64%\n",
            "Epoch  34/200, train loss: 2.07e-03, accuracy: 90.73%\n",
            "Epoch  34/200, val loss: 3.42e-03, accuracy: 87.22%\n",
            "Epoch  35/200, train loss: 2.08e-03, accuracy: 90.63%\n",
            "Epoch  35/200, val loss: 3.48e-03, accuracy: 86.42%\n",
            "Epoch  36/200, train loss: 2.02e-03, accuracy: 90.88%\n",
            "Epoch  36/200, val loss: 3.79e-03, accuracy: 85.04%\n",
            "Epoch  37/200, train loss: 1.99e-03, accuracy: 90.98%\n",
            "Epoch  37/200, val loss: 3.05e-03, accuracy: 87.98%\n",
            "Epoch  38/200, train loss: 1.93e-03, accuracy: 91.39%\n",
            "Epoch  38/200, val loss: 3.90e-03, accuracy: 85.58%\n",
            "Epoch  39/200, train loss: 1.94e-03, accuracy: 91.33%\n",
            "Epoch  39/200, val loss: 3.83e-03, accuracy: 85.72%\n",
            "Epoch  40/200, train loss: 1.93e-03, accuracy: 91.42%\n",
            "Epoch  40/200, val loss: 3.85e-03, accuracy: 85.18%\n",
            "Epoch  41/200, train loss: 1.91e-03, accuracy: 91.46%\n",
            "Epoch  41/200, val loss: 3.63e-03, accuracy: 85.96%\n",
            "Epoch  42/200, train loss: 1.88e-03, accuracy: 91.51%\n",
            "Epoch  42/200, val loss: 3.87e-03, accuracy: 86.70%\n",
            "Epoch  43/200, train loss: 1.81e-03, accuracy: 91.89%\n",
            "Epoch  43/200, val loss: 3.85e-03, accuracy: 85.62%\n",
            "Epoch  44/200, train loss: 1.87e-03, accuracy: 91.69%\n",
            "Epoch  44/200, val loss: 3.46e-03, accuracy: 86.94%\n",
            "Epoch  45/200, train loss: 1.81e-03, accuracy: 91.84%\n",
            "Epoch  45/200, val loss: 3.53e-03, accuracy: 86.34%\n",
            "Epoch  46/200, train loss: 1.80e-03, accuracy: 91.91%\n",
            "Epoch  46/200, val loss: 4.30e-03, accuracy: 84.80%\n",
            "Epoch  47/200, train loss: 1.81e-03, accuracy: 91.86%\n",
            "Epoch  47/200, val loss: 3.24e-03, accuracy: 88.30%\n",
            "Epoch  48/200, train loss: 1.81e-03, accuracy: 91.77%\n",
            "Epoch  48/200, val loss: 3.59e-03, accuracy: 86.80%\n",
            "Epoch  49/200, train loss: 1.74e-03, accuracy: 92.10%\n",
            "Epoch  49/200, val loss: 3.49e-03, accuracy: 87.06%\n",
            "Epoch  50/200, train loss: 1.75e-03, accuracy: 92.15%\n",
            "Epoch  50/200, val loss: 3.87e-03, accuracy: 85.14%\n",
            "Epoch  51/200, train loss: 1.74e-03, accuracy: 92.21%\n",
            "Epoch  51/200, val loss: 3.55e-03, accuracy: 86.76%\n",
            "Epoch  52/200, train loss: 1.73e-03, accuracy: 92.20%\n",
            "Epoch  52/200, val loss: 2.87e-03, accuracy: 89.20%\n",
            "Epoch  53/200, train loss: 1.70e-03, accuracy: 92.37%\n",
            "Epoch  53/200, val loss: 3.83e-03, accuracy: 87.02%\n",
            "Epoch  54/200, train loss: 1.72e-03, accuracy: 92.12%\n",
            "Epoch  54/200, val loss: 3.54e-03, accuracy: 86.60%\n",
            "Epoch  55/200, train loss: 1.69e-03, accuracy: 92.30%\n",
            "Epoch  55/200, val loss: 3.71e-03, accuracy: 86.14%\n",
            "Epoch  56/200, train loss: 1.68e-03, accuracy: 92.41%\n",
            "Epoch  56/200, val loss: 4.76e-03, accuracy: 84.16%\n",
            "Epoch  57/200, train loss: 1.67e-03, accuracy: 92.55%\n",
            "Epoch  57/200, val loss: 3.23e-03, accuracy: 88.02%\n",
            "Epoch  58/200, train loss: 1.67e-03, accuracy: 92.45%\n",
            "Epoch  58/200, val loss: 3.32e-03, accuracy: 87.66%\n",
            "Epoch  59/200, train loss: 1.66e-03, accuracy: 92.63%\n",
            "Epoch  59/200, val loss: 3.67e-03, accuracy: 86.98%\n",
            "Epoch  60/200, train loss: 1.62e-03, accuracy: 92.68%\n",
            "Epoch  60/200, val loss: 3.81e-03, accuracy: 85.86%\n",
            "Epoch  61/200, train loss: 1.65e-03, accuracy: 92.57%\n",
            "Epoch  61/200, val loss: 3.40e-03, accuracy: 89.06%\n",
            "Epoch  62/200, train loss: 1.57e-03, accuracy: 92.93%\n",
            "Epoch  62/200, val loss: 3.38e-03, accuracy: 87.26%\n",
            "Epoch  63/200, train loss: 1.61e-03, accuracy: 92.77%\n",
            "Epoch  63/200, val loss: 3.51e-03, accuracy: 87.48%\n",
            "Epoch  64/200, train loss: 1.64e-03, accuracy: 92.62%\n",
            "Epoch  64/200, val loss: 3.21e-03, accuracy: 88.24%\n",
            "Epoch  65/200, train loss: 1.55e-03, accuracy: 93.04%\n",
            "Epoch  65/200, val loss: 3.27e-03, accuracy: 87.94%\n",
            "Epoch  66/200, train loss: 1.57e-03, accuracy: 92.97%\n",
            "Epoch  66/200, val loss: 3.27e-03, accuracy: 87.64%\n",
            "Epoch  67/200, train loss: 1.54e-03, accuracy: 93.16%\n",
            "Epoch  67/200, val loss: 3.19e-03, accuracy: 88.38%\n",
            "Epoch  68/200, train loss: 1.57e-03, accuracy: 92.96%\n",
            "Epoch  68/200, val loss: 4.71e-03, accuracy: 84.14%\n",
            "Epoch  69/200, train loss: 1.54e-03, accuracy: 93.07%\n",
            "Epoch  69/200, val loss: 4.13e-03, accuracy: 85.44%\n",
            "Epoch  70/200, train loss: 1.53e-03, accuracy: 93.14%\n",
            "Epoch  70/200, val loss: 3.74e-03, accuracy: 86.92%\n",
            "Epoch  71/200, train loss: 1.57e-03, accuracy: 92.92%\n",
            "Epoch  71/200, val loss: 3.45e-03, accuracy: 87.10%\n",
            "Epoch  72/200, train loss: 1.52e-03, accuracy: 93.09%\n",
            "Epoch  72/200, val loss: 3.70e-03, accuracy: 86.88%\n",
            "Epoch  73/200, train loss: 1.51e-03, accuracy: 93.26%\n",
            "Epoch  73/200, val loss: 3.39e-03, accuracy: 87.48%\n",
            "Epoch  74/200, train loss: 1.51e-03, accuracy: 93.30%\n",
            "Epoch  74/200, val loss: 3.77e-03, accuracy: 86.16%\n",
            "Epoch  75/200, train loss: 1.49e-03, accuracy: 93.38%\n",
            "Epoch  75/200, val loss: 3.44e-03, accuracy: 87.80%\n",
            "Epoch  76/200, train loss: 1.51e-03, accuracy: 93.10%\n",
            "Epoch  76/200, val loss: 3.85e-03, accuracy: 86.66%\n",
            "Epoch  77/200, train loss: 1.46e-03, accuracy: 93.42%\n",
            "Epoch  77/200, val loss: 3.31e-03, accuracy: 87.82%\n",
            "Epoch  78/200, train loss: 1.48e-03, accuracy: 93.30%\n",
            "Epoch  78/200, val loss: 3.60e-03, accuracy: 87.38%\n",
            "Epoch  79/200, train loss: 1.52e-03, accuracy: 93.09%\n",
            "Epoch  79/200, val loss: 3.78e-03, accuracy: 87.30%\n",
            "Epoch  80/200, train loss: 1.43e-03, accuracy: 93.45%\n",
            "Epoch  80/200, val loss: 3.13e-03, accuracy: 88.36%\n",
            "Epoch  81/200, train loss: 1.47e-03, accuracy: 93.32%\n",
            "Epoch  81/200, val loss: 3.66e-03, accuracy: 86.48%\n",
            "Epoch  82/200, train loss: 1.47e-03, accuracy: 93.40%\n",
            "Epoch  82/200, val loss: 3.81e-03, accuracy: 86.98%\n",
            "Epoch  83/200, train loss: 1.42e-03, accuracy: 93.55%\n",
            "Epoch  83/200, val loss: 3.70e-03, accuracy: 86.36%\n",
            "Epoch  84/200, train loss: 1.46e-03, accuracy: 93.43%\n",
            "Epoch  84/200, val loss: 5.10e-03, accuracy: 84.18%\n",
            "Epoch  85/200, train loss: 1.45e-03, accuracy: 93.38%\n",
            "Epoch  85/200, val loss: 3.03e-03, accuracy: 88.60%\n",
            "Epoch  86/200, train loss: 1.44e-03, accuracy: 93.48%\n",
            "Epoch  86/200, val loss: 4.08e-03, accuracy: 86.34%\n",
            "Epoch  87/200, train loss: 1.46e-03, accuracy: 93.32%\n",
            "Epoch  87/200, val loss: 3.28e-03, accuracy: 87.84%\n",
            "Epoch  88/200, train loss: 1.42e-03, accuracy: 93.57%\n",
            "Epoch  88/200, val loss: 4.44e-03, accuracy: 85.40%\n",
            "Epoch  89/200, train loss: 1.44e-03, accuracy: 93.44%\n",
            "Epoch  89/200, val loss: 2.88e-03, accuracy: 89.16%\n",
            "Epoch  90/200, train loss: 1.46e-03, accuracy: 93.46%\n",
            "Epoch  90/200, val loss: 3.73e-03, accuracy: 86.24%\n",
            "Epoch  91/200, train loss: 1.41e-03, accuracy: 93.56%\n",
            "Epoch  91/200, val loss: 3.77e-03, accuracy: 86.72%\n",
            "Epoch  92/200, train loss: 1.41e-03, accuracy: 93.65%\n",
            "Epoch  92/200, val loss: 3.63e-03, accuracy: 87.54%\n",
            "Epoch  93/200, train loss: 1.38e-03, accuracy: 93.84%\n",
            "Epoch  93/200, val loss: 2.89e-03, accuracy: 89.22%\n",
            "Epoch  94/200, train loss: 1.41e-03, accuracy: 93.83%\n",
            "Epoch  94/200, val loss: 3.71e-03, accuracy: 86.74%\n",
            "Epoch  95/200, train loss: 1.43e-03, accuracy: 93.60%\n",
            "Epoch  95/200, val loss: 2.83e-03, accuracy: 89.56%\n",
            "Epoch  96/200, train loss: 1.40e-03, accuracy: 93.73%\n",
            "Epoch  96/200, val loss: 3.37e-03, accuracy: 87.62%\n",
            "Epoch  97/200, train loss: 1.41e-03, accuracy: 93.60%\n",
            "Epoch  97/200, val loss: 3.16e-03, accuracy: 88.44%\n",
            "Epoch  98/200, train loss: 1.39e-03, accuracy: 93.68%\n",
            "Epoch  98/200, val loss: 3.32e-03, accuracy: 88.40%\n",
            "Epoch  99/200, train loss: 1.38e-03, accuracy: 93.86%\n",
            "Epoch  99/200, val loss: 3.75e-03, accuracy: 87.36%\n",
            "Epoch 100/200, train loss: 1.41e-03, accuracy: 93.70%\n",
            "Epoch 100/200, val loss: 3.82e-03, accuracy: 86.66%\n",
            "Epoch 101/200, train loss: 7.66e-04, accuracy: 96.73%\n",
            "Epoch 101/200, val loss: 2.14e-03, accuracy: 92.28%\n",
            "Epoch 102/200, train loss: 5.57e-04, accuracy: 97.71%\n",
            "Epoch 102/200, val loss: 2.12e-03, accuracy: 92.54%\n",
            "Epoch 103/200, train loss: 4.60e-04, accuracy: 98.10%\n",
            "Epoch 103/200, val loss: 2.14e-03, accuracy: 92.46%\n",
            "Epoch 104/200, train loss: 4.23e-04, accuracy: 98.29%\n",
            "Epoch 104/200, val loss: 2.12e-03, accuracy: 92.66%\n",
            "Epoch 105/200, train loss: 3.85e-04, accuracy: 98.44%\n",
            "Epoch 105/200, val loss: 2.12e-03, accuracy: 92.98%\n",
            "Epoch 106/200, train loss: 3.60e-04, accuracy: 98.55%\n",
            "Epoch 106/200, val loss: 2.20e-03, accuracy: 92.90%\n",
            "Epoch 107/200, train loss: 3.26e-04, accuracy: 98.72%\n",
            "Epoch 107/200, val loss: 2.23e-03, accuracy: 92.90%\n",
            "Epoch 108/200, train loss: 2.95e-04, accuracy: 98.86%\n",
            "Epoch 108/200, val loss: 2.25e-03, accuracy: 92.98%\n",
            "Epoch 109/200, train loss: 2.72e-04, accuracy: 98.93%\n",
            "Epoch 109/200, val loss: 2.26e-03, accuracy: 93.04%\n",
            "Epoch 110/200, train loss: 2.64e-04, accuracy: 98.96%\n",
            "Epoch 110/200, val loss: 2.26e-03, accuracy: 93.18%\n",
            "Epoch 111/200, train loss: 2.49e-04, accuracy: 99.02%\n",
            "Epoch 111/200, val loss: 2.37e-03, accuracy: 92.96%\n",
            "Epoch 112/200, train loss: 2.24e-04, accuracy: 99.19%\n",
            "Epoch 112/200, val loss: 2.38e-03, accuracy: 93.02%\n",
            "Epoch 113/200, train loss: 2.32e-04, accuracy: 99.10%\n",
            "Epoch 113/200, val loss: 2.33e-03, accuracy: 93.14%\n",
            "Epoch 114/200, train loss: 2.13e-04, accuracy: 99.23%\n",
            "Epoch 114/200, val loss: 2.38e-03, accuracy: 92.90%\n",
            "Epoch 115/200, train loss: 2.14e-04, accuracy: 99.19%\n",
            "Epoch 115/200, val loss: 2.40e-03, accuracy: 92.86%\n",
            "Epoch 116/200, train loss: 2.01e-04, accuracy: 99.22%\n",
            "Epoch 116/200, val loss: 2.43e-03, accuracy: 92.92%\n",
            "Epoch 117/200, train loss: 1.93e-04, accuracy: 99.25%\n",
            "Epoch 117/200, val loss: 2.46e-03, accuracy: 92.92%\n",
            "Epoch 118/200, train loss: 1.82e-04, accuracy: 99.33%\n",
            "Epoch 118/200, val loss: 2.48e-03, accuracy: 92.90%\n",
            "Epoch 119/200, train loss: 1.78e-04, accuracy: 99.30%\n",
            "Epoch 119/200, val loss: 2.51e-03, accuracy: 92.92%\n",
            "Epoch 120/200, train loss: 1.54e-04, accuracy: 99.47%\n",
            "Epoch 120/200, val loss: 2.52e-03, accuracy: 92.92%\n",
            "Epoch 121/200, train loss: 1.73e-04, accuracy: 99.34%\n",
            "Epoch 121/200, val loss: 2.58e-03, accuracy: 93.00%\n",
            "Epoch 122/200, train loss: 1.59e-04, accuracy: 99.41%\n",
            "Epoch 122/200, val loss: 2.57e-03, accuracy: 92.74%\n",
            "Epoch 123/200, train loss: 1.54e-04, accuracy: 99.44%\n",
            "Epoch 123/200, val loss: 2.51e-03, accuracy: 92.88%\n",
            "Epoch 124/200, train loss: 1.49e-04, accuracy: 99.46%\n",
            "Epoch 124/200, val loss: 2.58e-03, accuracy: 92.90%\n",
            "Epoch 125/200, train loss: 1.37e-04, accuracy: 99.53%\n",
            "Epoch 125/200, val loss: 2.62e-03, accuracy: 93.12%\n",
            "Epoch 126/200, train loss: 1.37e-04, accuracy: 99.50%\n",
            "Epoch 126/200, val loss: 2.53e-03, accuracy: 92.82%\n",
            "Epoch 127/200, train loss: 1.35e-04, accuracy: 99.47%\n",
            "Epoch 127/200, val loss: 2.61e-03, accuracy: 92.94%\n",
            "Epoch 128/200, train loss: 1.33e-04, accuracy: 99.52%\n",
            "Epoch 128/200, val loss: 2.65e-03, accuracy: 92.94%\n",
            "Epoch 129/200, train loss: 1.30e-04, accuracy: 99.54%\n",
            "Epoch 129/200, val loss: 2.66e-03, accuracy: 92.92%\n",
            "Epoch 130/200, train loss: 1.23e-04, accuracy: 99.56%\n",
            "Epoch 130/200, val loss: 2.63e-03, accuracy: 92.98%\n",
            "Epoch 131/200, train loss: 1.20e-04, accuracy: 99.61%\n",
            "Epoch 131/200, val loss: 2.59e-03, accuracy: 92.94%\n",
            "Epoch 132/200, train loss: 1.15e-04, accuracy: 99.60%\n",
            "Epoch 132/200, val loss: 2.64e-03, accuracy: 92.96%\n",
            "Epoch 133/200, train loss: 1.13e-04, accuracy: 99.60%\n",
            "Epoch 133/200, val loss: 2.75e-03, accuracy: 92.92%\n",
            "Epoch 134/200, train loss: 1.14e-04, accuracy: 99.59%\n",
            "Epoch 134/200, val loss: 2.67e-03, accuracy: 93.18%\n",
            "Epoch 135/200, train loss: 1.08e-04, accuracy: 99.61%\n",
            "Epoch 135/200, val loss: 2.60e-03, accuracy: 93.24%\n",
            "Epoch 136/200, train loss: 1.08e-04, accuracy: 99.64%\n",
            "Epoch 136/200, val loss: 2.72e-03, accuracy: 93.24%\n",
            "Epoch 137/200, train loss: 1.02e-04, accuracy: 99.64%\n",
            "Epoch 137/200, val loss: 2.75e-03, accuracy: 93.02%\n",
            "Epoch 138/200, train loss: 1.01e-04, accuracy: 99.66%\n",
            "Epoch 138/200, val loss: 2.63e-03, accuracy: 93.24%\n",
            "Epoch 139/200, train loss: 1.02e-04, accuracy: 99.62%\n",
            "Epoch 139/200, val loss: 2.74e-03, accuracy: 93.08%\n",
            "Epoch 140/200, train loss: 9.49e-05, accuracy: 99.71%\n",
            "Epoch 140/200, val loss: 2.63e-03, accuracy: 93.30%\n",
            "Epoch 141/200, train loss: 9.91e-05, accuracy: 99.68%\n",
            "Epoch 141/200, val loss: 2.68e-03, accuracy: 93.38%\n",
            "Epoch 142/200, train loss: 9.47e-05, accuracy: 99.69%\n",
            "Epoch 142/200, val loss: 2.77e-03, accuracy: 92.98%\n",
            "Epoch 143/200, train loss: 9.18e-05, accuracy: 99.68%\n",
            "Epoch 143/200, val loss: 2.70e-03, accuracy: 93.02%\n",
            "Epoch 144/200, train loss: 9.03e-05, accuracy: 99.71%\n",
            "Epoch 144/200, val loss: 2.69e-03, accuracy: 93.04%\n",
            "Epoch 145/200, train loss: 8.89e-05, accuracy: 99.70%\n",
            "Epoch 145/200, val loss: 2.86e-03, accuracy: 92.88%\n",
            "Epoch 146/200, train loss: 8.32e-05, accuracy: 99.74%\n",
            "Epoch 146/200, val loss: 2.72e-03, accuracy: 93.20%\n",
            "Epoch 147/200, train loss: 8.30e-05, accuracy: 99.72%\n",
            "Epoch 147/200, val loss: 2.73e-03, accuracy: 93.08%\n",
            "Epoch 148/200, train loss: 8.09e-05, accuracy: 99.73%\n",
            "Epoch 148/200, val loss: 2.77e-03, accuracy: 93.26%\n",
            "Epoch 149/200, train loss: 8.05e-05, accuracy: 99.74%\n",
            "Epoch 149/200, val loss: 2.85e-03, accuracy: 92.88%\n",
            "Epoch 150/200, train loss: 8.69e-05, accuracy: 99.70%\n",
            "Epoch 150/200, val loss: 2.81e-03, accuracy: 92.80%\n",
            "Epoch 151/200, train loss: 7.75e-05, accuracy: 99.76%\n",
            "Epoch 151/200, val loss: 2.75e-03, accuracy: 92.88%\n",
            "Epoch 152/200, train loss: 7.54e-05, accuracy: 99.78%\n",
            "Epoch 152/200, val loss: 2.74e-03, accuracy: 93.04%\n",
            "Epoch 153/200, train loss: 6.42e-05, accuracy: 99.83%\n",
            "Epoch 153/200, val loss: 2.72e-03, accuracy: 93.10%\n",
            "Epoch 154/200, train loss: 6.93e-05, accuracy: 99.81%\n",
            "Epoch 154/200, val loss: 2.74e-03, accuracy: 93.04%\n",
            "Epoch 155/200, train loss: 6.35e-05, accuracy: 99.84%\n",
            "Epoch 155/200, val loss: 2.73e-03, accuracy: 93.04%\n",
            "Epoch 156/200, train loss: 6.39e-05, accuracy: 99.80%\n",
            "Epoch 156/200, val loss: 2.70e-03, accuracy: 93.22%\n",
            "Epoch 157/200, train loss: 6.60e-05, accuracy: 99.81%\n",
            "Epoch 157/200, val loss: 2.74e-03, accuracy: 93.22%\n",
            "Epoch 158/200, train loss: 5.97e-05, accuracy: 99.85%\n",
            "Epoch 158/200, val loss: 2.70e-03, accuracy: 93.22%\n",
            "Epoch 159/200, train loss: 5.33e-05, accuracy: 99.86%\n",
            "Epoch 159/200, val loss: 2.71e-03, accuracy: 93.16%\n",
            "Epoch 160/200, train loss: 6.38e-05, accuracy: 99.82%\n",
            "Epoch 160/200, val loss: 2.65e-03, accuracy: 93.30%\n",
            "Epoch 161/200, train loss: 5.73e-05, accuracy: 99.85%\n",
            "Epoch 161/200, val loss: 2.71e-03, accuracy: 93.22%\n",
            "Epoch 162/200, train loss: 5.83e-05, accuracy: 99.84%\n",
            "Epoch 162/200, val loss: 2.67e-03, accuracy: 93.26%\n",
            "Epoch 163/200, train loss: 5.85e-05, accuracy: 99.85%\n",
            "Epoch 163/200, val loss: 2.70e-03, accuracy: 93.20%\n",
            "Epoch 164/200, train loss: 5.38e-05, accuracy: 99.86%\n",
            "Epoch 164/200, val loss: 2.70e-03, accuracy: 93.20%\n",
            "Epoch 165/200, train loss: 5.26e-05, accuracy: 99.88%\n",
            "Epoch 165/200, val loss: 2.71e-03, accuracy: 93.20%\n",
            "Epoch 166/200, train loss: 5.53e-05, accuracy: 99.86%\n",
            "Epoch 166/200, val loss: 2.71e-03, accuracy: 93.16%\n",
            "Epoch 167/200, train loss: 5.74e-05, accuracy: 99.85%\n",
            "Epoch 167/200, val loss: 2.72e-03, accuracy: 93.30%\n",
            "Epoch 168/200, train loss: 5.66e-05, accuracy: 99.85%\n",
            "Epoch 168/200, val loss: 2.70e-03, accuracy: 93.36%\n",
            "Epoch 169/200, train loss: 5.85e-05, accuracy: 99.82%\n",
            "Epoch 169/200, val loss: 2.70e-03, accuracy: 93.16%\n",
            "Epoch 170/200, train loss: 5.41e-05, accuracy: 99.89%\n",
            "Epoch 170/200, val loss: 2.72e-03, accuracy: 93.36%\n",
            "Epoch 171/200, train loss: 5.31e-05, accuracy: 99.85%\n",
            "Epoch 171/200, val loss: 2.73e-03, accuracy: 93.18%\n",
            "Epoch 172/200, train loss: 5.53e-05, accuracy: 99.87%\n",
            "Epoch 172/200, val loss: 2.69e-03, accuracy: 93.22%\n",
            "Epoch 173/200, train loss: 5.10e-05, accuracy: 99.88%\n",
            "Epoch 173/200, val loss: 2.68e-03, accuracy: 93.28%\n",
            "Epoch 174/200, train loss: 5.35e-05, accuracy: 99.88%\n",
            "Epoch 174/200, val loss: 2.75e-03, accuracy: 93.12%\n",
            "Epoch 175/200, train loss: 5.81e-05, accuracy: 99.84%\n",
            "Epoch 175/200, val loss: 2.68e-03, accuracy: 93.28%\n",
            "Epoch 176/200, train loss: 4.82e-05, accuracy: 99.89%\n",
            "Epoch 176/200, val loss: 2.68e-03, accuracy: 93.24%\n",
            "Epoch 177/200, train loss: 4.76e-05, accuracy: 99.89%\n",
            "Epoch 177/200, val loss: 2.68e-03, accuracy: 93.28%\n",
            "Epoch 178/200, train loss: 5.09e-05, accuracy: 99.88%\n",
            "Epoch 178/200, val loss: 2.71e-03, accuracy: 93.26%\n",
            "Epoch 179/200, train loss: 4.88e-05, accuracy: 99.89%\n",
            "Epoch 179/200, val loss: 2.73e-03, accuracy: 93.20%\n",
            "Epoch 180/200, train loss: 5.28e-05, accuracy: 99.88%\n",
            "Epoch 180/200, val loss: 2.69e-03, accuracy: 93.18%\n",
            "Epoch 181/200, train loss: 4.76e-05, accuracy: 99.90%\n",
            "Epoch 181/200, val loss: 2.69e-03, accuracy: 93.18%\n",
            "Epoch 182/200, train loss: 5.26e-05, accuracy: 99.88%\n",
            "Epoch 182/200, val loss: 2.69e-03, accuracy: 93.30%\n",
            "Epoch 183/200, train loss: 4.78e-05, accuracy: 99.89%\n",
            "Epoch 183/200, val loss: 2.68e-03, accuracy: 93.18%\n",
            "Epoch 184/200, train loss: 5.32e-05, accuracy: 99.86%\n",
            "Epoch 184/200, val loss: 2.70e-03, accuracy: 93.08%\n",
            "Epoch 185/200, train loss: 5.02e-05, accuracy: 99.89%\n",
            "Epoch 185/200, val loss: 2.71e-03, accuracy: 93.26%\n",
            "Epoch 186/200, train loss: 5.24e-05, accuracy: 99.86%\n",
            "Epoch 186/200, val loss: 2.74e-03, accuracy: 93.20%\n",
            "Epoch 187/200, train loss: 5.38e-05, accuracy: 99.85%\n",
            "Epoch 187/200, val loss: 2.72e-03, accuracy: 93.18%\n",
            "Epoch 188/200, train loss: 5.22e-05, accuracy: 99.88%\n",
            "Epoch 188/200, val loss: 2.74e-03, accuracy: 93.14%\n",
            "Epoch 189/200, train loss: 5.19e-05, accuracy: 99.85%\n",
            "Epoch 189/200, val loss: 2.70e-03, accuracy: 93.28%\n",
            "Epoch 190/200, train loss: 5.01e-05, accuracy: 99.85%\n",
            "Epoch 190/200, val loss: 2.71e-03, accuracy: 93.30%\n",
            "Epoch 191/200, train loss: 5.23e-05, accuracy: 99.86%\n",
            "Epoch 191/200, val loss: 2.77e-03, accuracy: 93.12%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfXZwKoNnVWP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "85955a12-145c-4b38-cb21-56c045a68bc2"
      },
      "source": [
        "test_loss, test_acc = run_epoch(resnet, None, dataloaders['test'], train=False)\n",
        "print(f\"Test loss: {test_loss:.1e}, accuracy: {test_acc * 100:.2f}%\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 2.9e-03, accuracy: 92.20%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}